"""src/preprocess.py"""

import re
from hazm import Normalizer, word_tokenize, Stemmer

# A list of stop words (you can expand it).
# hazm.stopwords_list() can also be used, but it might remove some useful keywords.
STOP_WORDS = set([
    'Ùˆ', 'Ø¯Ø±', 'Ø¨Ù‡', 'Ø§Ø²', 'Ú©Ù‡', 'Ø§ÛŒÙ†', 'Ø¢Ù†', 'ÛŒÚ©', 'Ø¨Ø§', 'Ø§Ø³Øª', 'Ø¨Ø±Ø§ÛŒ', 
    'ØªØ§', 'Ù‡Ù…', 'Ù†ÛŒØ²', 'Ù…Ù†', 'ØªÙˆ', 'Ø§Ùˆ', 'Ù…Ø§', 'Ø´Ù…Ø§', 'Ø§ÛŒØ´Ø§Ù†', 'Ù‡Ø±',
    'Ù‡Ù…Ù‡', 'ÛŒÚ©', 'Ø¯ÛŒÚ¯Ø±', 'Ø¨Ø±Ø®ÛŒ', 'Ø±ÙˆÛŒ', 'Ø²ÛŒØ±', 'Ø¨Ø§Ù„Ø§', 'Ù¾Ø§ÛŒÛŒÙ†'
])

def clean_text(text):
    """
    A function to normalize, tokenize, remove stop words, and stem the text.
    """
    # 1. Normalize text
    normalizer = Normalizer()
    text = normalizer.normalize(text)
    
    # Remove unnecessary characters (like emojis, extra punctuation)
    text = re.sub(r'[^\w\s]', '', text)
    
    # 2. Tokenize (split text into words)
    tokens = word_tokenize(text)
    
    # 3. Remove stop words
    tokens = [word for word in tokens if word not in STOP_WORDS]
    
    # 4. Stem words (optional but useful)
    stemmer = Stemmer()
    tokens = [stemmer.stem(word) for word in tokens]
    
    # Join the tokens back into a single string
    cleaned_text = ' '.join(tokens)
    
    return cleaned_text

if __name__ == '__main__':
    # An example to test the function
    sample_text = "Ù…Ù† Ø§Ø² Ø§ÛŒÙ† Ù…Ø­ØµÙˆÙ„ Ø§ØµÙ„Ø§ Ø±Ø§Ø¶ÛŒ Ù†ÛŒØ³ØªÙ… Ùˆ Ø®ÛŒÙ„ÛŒ Ø¨ÛŒâ€ŒÚ©ÛŒÙÛŒØª Ø¨ÙˆØ¯!! ğŸ˜ "
    cleaned = clean_text(sample_text)
    print(f"Original: {sample_text}")
    print(f"Cleaned: {cleaned}")
